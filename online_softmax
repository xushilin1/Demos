import numpy as np

def flash_attention_single_query(q, K_blocks, V_blocks):
    """
    q: (d,)
    K_blocks: list of (B_i, d)
    V_blocks: list of (B_i, d_v)
    """
    d = q.shape[0]

    # running statistics
    m = -np.inf          # running max
    l = 0.0              # running sum exp
    o = np.zeros(V_blocks[0].shape[1])  # running output

    for K, V in zip(K_blocks, V_blocks):
        # 1. compute scores for this block
        scores = (K @ q) / np.sqrt(d)   # (B_i,)

        # 2. block max
        block_max = scores.max()

        # 3. update global max
        new_m = max(m, block_max)

        # 4. rescale previous accumulations
        exp_scale = np.exp(m - new_m)
        l *= exp_scale
        o *= exp_scale

        # 5. accumulate current block
        exp_scores = np.exp(scores - new_m)   # safe exp
        l += exp_scores.sum()
        o += exp_scores @ V

        # 6. move forward
        m = new_m

    return o / l
