import torch
import torch.distributed as dist
import os

def init_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    rank = int(os.environ['RANK'])
    world_size = int(os.environ['WORLD_SIZE'])
    local_rank = int(os.environ['LOCAL_RANK'])
    assert world_size == 2
    
    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group(backend='nccl')
    
    return rank, world_size, local_rank

def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    dist.destroy_process_group()

def test_all_reduce():
    """
    æµ‹è¯•PyTorchçš„all_reduceæ“ä½œã€‚
    
    1. åœ¨æ‰€æœ‰è¿›ç¨‹é—´æ‰§è¡ŒæŒ‡å®šçš„è§„çº¦æ“ä½œï¼ˆå¦‚æ±‚å’Œã€æ±‚å¹³å‡ã€æ±‚æœ€å¤§å€¼ç­‰ï¼‰
    2. ç¡®ä¿æ‰€æœ‰è¿›ç¨‹æœ€ç»ˆå¾—åˆ°ç›¸åŒçš„ç»“æœ
    
    å…·ä½“è¿‡ç¨‹ï¼š
    - æ¯ä¸ªè¿›ç¨‹æä¾›ä¸€ä¸ªè¾“å…¥å¼ é‡
    - æ‰€æœ‰è¿›ç¨‹çš„å¼ é‡æŒ‰ç…§æŒ‡å®šçš„æ“ä½œè¿›è¡Œè§„çº¦ï¼ˆæœ¬ä¾‹ä¸­ä½¿ç”¨æ±‚å’Œï¼‰
    - ç»“æœè¢«å¹¿æ’­å›æ‰€æœ‰è¿›ç¨‹
    - æ‰€æœ‰è¿›ç¨‹å¾—åˆ°å®Œå…¨ç›¸åŒçš„è¾“å‡ºå¼ é‡
    """

    rank = dist.get_rank()
    if rank == 0:
        print('=' * 60)
        print(f"[TEST_ALL_REDUCE]")
    
    torch.set_default_device(f"cuda:{rank}")
    if rank == 0:
        tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)
    else:
        tensor = torch.tensor([4.0, 5.0, 6.0], dtype=torch.float32)
    
    print(f"è¿›ç¨‹ {rank}: åˆå§‹å¼ é‡ = {tensor}")
    
    # æ‰§è¡Œall_reduceæ“ä½œ - æ‰€æœ‰è¿›ç¨‹çš„å¼ é‡ä¼šè¢«æ±‚å’Œ
    # åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½ä¼šç›¸äº’é€šä¿¡ï¼Œæœ€ç»ˆæ¯ä¸ªè¿›ç¨‹å¾—åˆ°ç›¸åŒçš„ç»“æœ
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM) # [5, 7, 9]
    # dist.all_reduce(tensor, op=dist.ReduceOp.MAX) # [4, 5, 6]
    # dist.all_reduce(tensor, op=dist.ReduceOp.AVG) # [2.5, 3.5, 4.5]
    
    if rank == 0:
        print(f"æœŸæœ›ç»“æœ = {tensor}")
        print('=' * 60)

def test_broadcast():
    """æµ‹è¯•broadcastæ“ä½œ"""
    rank = dist.get_rank()
    torch.set_default_device(f"cuda:{rank}")
    
    if rank == 0:
        print('=' * 60)
        print(f"[TEST_BROADCAST]")
    
    if rank == 0:
        data = torch.tensor([10.0, 20.0, 30.0])
        print(f"Rootè¿›ç¨‹ {rank} å¹¿æ’­æ•°æ®: {data}")
    else:
        data = torch.tensor([1.0, 2.0, 3.0])
        print(f"è¿›ç¨‹ {rank} ç­‰å¾…æ¥æ”¶æ•°æ®")
    
    dist.broadcast(data, src=0)
    if rank == 1:
        print(f"è¿›ç¨‹ {rank} æ¥æ”¶åˆ°çš„æ•°æ®: {data}")
        print('=' * 60)

def test_reduce():
    """
    æµ‹è¯•reduceæ“ä½œï¼šå°†å¤šä¸ªè¿›ç¨‹çš„æ•°æ®è§„çº¦(reduce)åˆ°ä¸€ä¸ªç›®æ ‡è¿›ç¨‹
    
    Reduceæ“ä½œçš„ç‰¹ç‚¹ï¼š
    - æ‰€æœ‰è¿›ç¨‹æä¾›è¾“å…¥æ•°æ®
    - åœ¨ç›®æ ‡è¿›ç¨‹(dst)ä¸Šæ‰§è¡ŒæŒ‡å®šçš„è§„çº¦æ“ä½œ(å¦‚æ±‚å’Œã€æ±‚å¹³å‡ç­‰)
    - åªæœ‰ç›®æ ‡è¿›ç¨‹æ¥æ”¶åˆ°æœ€ç»ˆç»“æœï¼Œå…¶ä»–è¿›ç¨‹çš„æ•°æ®ä¼šè¢«æ›´æ–°ä½†ä¸ä¸€å®šæœ‰æ„ä¹‰
    
    ä¸all_reduceçš„åŒºåˆ«ï¼š
    - all_reduce: æ‰€æœ‰è¿›ç¨‹éƒ½å¾—åˆ°è§„çº¦ç»“æœ
    - reduce: åªæœ‰æŒ‡å®šçš„ç›®æ ‡è¿›ç¨‹å¾—åˆ°ç»“æœ
    """
    rank = dist.get_rank()
    torch.set_default_device(f"cuda:{rank}")
    
    if rank == 0:
        print('=' * 60)
        print(f"[TEST_REDUCE]")
    
    if rank == 0:
        tensor = torch.tensor([1, 2, 3])
    else:
        tensor = torch.tensor([4, 5, 6])

    print(f"è¿›ç¨‹ {rank}: åˆå§‹å¼ é‡ = {tensor}")
    
    # æ‰§è¡Œreduceæ“ä½œï¼šå°†æ‰€æœ‰è¿›ç¨‹çš„æ•°æ®æ±‚å’Œåˆ°è¿›ç¨‹0
    dist.reduce(tensor, dst=0, op=dist.ReduceOp.SUM)
    
    if rank == 0:
        print(f"ğŸ“ Rootè¿›ç¨‹æ¥æ”¶åˆ°çš„è§„çº¦ç»“æœ: {tensor}")
        print('=' * 60)

def test_scatter_gather():
    """
    æµ‹è¯•scatterå’Œgatheræ“ä½œï¼šç”¨äºæ•°æ®åˆ†å‘ä¸æ”¶é›†çš„ç»å…¸ç»„åˆ
    
    Scatterï¼ˆåˆ†å‘ï¼‰æ“ä½œï¼š
    - srcè¿›ç¨‹å°†ä¸€ä¸ªåˆ—è¡¨çš„æ•°æ®åˆ†å‘åˆ°æ‰€æœ‰è¿›ç¨‹ï¼ˆåŒ…æ‹¬è‡ªèº«ï¼‰
    - æ¯ä¸ªè¿›ç¨‹æ¥æ”¶å¯¹åº”çš„ä¸€ä¸ªæ•°æ®ç‰‡æ®µ
    
    æœ¬æµ‹è¯•å®Œæ•´æµç¨‹ï¼š
    1. Scatteré˜¶æ®µï¼š
       - è¿›ç¨‹0: scatter_list = [[1.0, 2.0], [3.0, 4.0]]
       - è¿›ç¨‹0æ¥æ”¶ï¼š[1.0, 2.0]ï¼Œè¿›ç¨‹1æ¥æ”¶ï¼š[3.0, 4.0]

    2. Gatheré˜¶æ®µï¼š
       - è¿›ç¨‹0æ”¶é›†ï¼š[[1.0, 2.0], [6.0, 8.0]]
    
    """
    rank = dist.get_rank()
    torch.set_default_device(f"cuda:{rank}")
    
    if rank == 0:
        print('=' * 60)
        print("æµ‹è¯•æ•°æ®åˆ†å‘ä¸æ”¶é›†æ“ä½œ")
    
    # Scatteræµ‹è¯•ï¼šrootè¿›ç¨‹åˆ†å‘æ•°æ®åˆ°å„è¿›ç¨‹
    if rank == 0:
        scatter_list = [
            torch.tensor([1.0, 2.0]),
            torch.tensor([3.0, 4.0])
        ]
    else:
        scatter_list = None
    
    recv_tensor = torch.zeros(2)
    dist.scatter(recv_tensor, scatter_list, src=0)
    print(f"è¿›ç¨‹{rank} scatteræ¥æ”¶: {recv_tensor}")
    
    # Gatheræµ‹è¯•ï¼šæ‰€æœ‰è¿›ç¨‹æ”¶é›†æ•°æ®åˆ°rootè¿›ç¨‹
    send_tensor = recv_tensor * (rank + 1)
    gather_list = [torch.zeros(2) for _ in range(2)] if rank == 0 else None
    dist.gather(send_tensor, gather_list, dst=0)

    if rank == 0:
        print(f"è¿›ç¨‹{rank} gatherå‘é€: {send_tensor}")
        print(f"è¿›ç¨‹{rank} gatheræ¥æ”¶: {gather_list}")
    
        print('=' * 60)

if __name__ == "__main__":
    init_distributed()
    try:
        test_all_reduce()
        test_broadcast()
        test_reduce()
        test_scatter_gather()
    finally:
        cleanup_distributed()